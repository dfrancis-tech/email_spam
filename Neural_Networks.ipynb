{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f20e8973",
   "metadata": {},
   "source": [
    "# Spam Email detection Neural Networks\n",
    "Prepared By Deepa Francis<br>\n",
    "For BrainStation<br>\n",
    "On July 31, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc6244",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "[1. Configuring Resources](#cr) <br>\n",
    "- [1.1. Set up Libraries](#sl) <br>\n",
    "- [1.2. Load Data](#ld) <br>\n",
    "[2. Neural Network](#nn) <br>\n",
    "- [2.1. Architecture](#nnr) <br>\n",
    "- [2.2. TF-IDF model evaluation](#tf) <br>\n",
    "- [2.3. Sentence2vec model evaluation](#sv) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbb0df9",
   "metadata": {},
   "source": [
    "<a id = \"cr\"></a>\n",
    "## 1. Configuring Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737cc84f",
   "metadata": {},
   "source": [
    "We are going to configure resources for comparing the performance metrics of neural network models on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500a7849",
   "metadata": {},
   "source": [
    "<a id = \"sl\"></a>\n",
    "### 1.1. Setting up Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bfeef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e82170",
   "metadata": {},
   "source": [
    "<a id = \"ld\"></a>\n",
    "### 1.2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b593a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "X_train = pd.read_csv('X_train.csv') \n",
    "X_test = pd.read_csv('X_test.csv') \n",
    "X_validation = pd.read_csv('X_validation.csv') \n",
    "\n",
    "X_train_Vec = pd.read_csv('X_train_Vec.csv') \n",
    "X_test_Vec = pd.read_csv('X_test_Vec.csv') \n",
    "X_val_Vec = pd.read_csv('X_val_Vec.csv')\n",
    "\n",
    "y_train = pd.read_csv('y_train.csv') \n",
    "y_test = pd.read_csv('y_test.csv') \n",
    "y_validation = pd.read_csv('y_validation.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef7afa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is (22400, 228)\n",
      "The shape of X_test is (12000, 228)\n",
      "The shape of X_validation is (5600, 228)\n",
      "\n",
      "The shape of X_train_Vec is (22400, 428)\n",
      "The shape of X_test_Vec is (12000, 428)\n",
      "The shape of X_val_Vec is (5600, 428)\n",
      "\n",
      "The shape of y_train is (22400, 1)\n",
      "The shape of y_test is (12000, 1)\n",
      "The shape of y_validation is (5600, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check the shapes of the datasets\n",
    "print(f'The shape of X_train is {X_train.shape}')\n",
    "print(f'The shape of X_test is {X_test.shape}')\n",
    "print(f'The shape of X_validation is {X_validation.shape}')\n",
    "print('')\n",
    "print(f'The shape of X_train_Vec is {X_train_Vec.shape}')\n",
    "print(f'The shape of X_test_Vec is {X_test_Vec.shape}')\n",
    "print(f'The shape of X_val_Vec is {X_val_Vec.shape}')\n",
    "print('')\n",
    "print(f'The shape of y_train is {y_train.shape}')\n",
    "print(f'The shape of y_test is {y_test.shape}')\n",
    "print(f'The shape of y_validation is {y_validation.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90972be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the scaler on X_train\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform X_test and X_validation using the previously fitted scaler\n",
    "X_test = scaler.transform(X_test)\n",
    "X_validation = scaler.transform(X_validation)\n",
    "\n",
    "# Similarly, transform the vectorized data\n",
    "X_train_Vec = scaler.fit_transform(X_train_Vec)\n",
    "X_test_Vec = scaler.transform(X_test_Vec)\n",
    "X_val_Vec = scaler.transform(X_val_Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4d6dca",
   "metadata": {},
   "source": [
    "<a id = \"nn\"></a>\n",
    "## 2. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8852c8b4",
   "metadata": {},
   "source": [
    "The main objective is to compare the performance of the neural network when using two different text representation methods: TF-IDF Vectorizer and Sentence to Vec. The comparison will involve training the neural network with data represented using both approaches and evaluating its performance using the chosen performance metrics.\n",
    "\n",
    "Each approach may have its strengths and weaknesses depending on the nature of the text data and the complexity of the task at hand. By evaluating the neural network's performance under both methods, we can determine which representation technique yields better results for the given scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9992038",
   "metadata": {},
   "source": [
    "<a id = \"nnr\"></a>\n",
    "### 2.1. Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69caa039",
   "metadata": {},
   "source": [
    "**Create a Neural Network**\n",
    "- First we create a sequential model. A sequential model is a linear stack of layers. In this case, the model will be built layer-by-layer.\n",
    "\n",
    "- The model has three hidden layers, each followed by dropout regularization and batch normalization.\n",
    "\n",
    "    - Dense: The dense layer is a fully connected layer with 40 neurons. The activation function used is ReLU (Rectified Linear Unit), which helps introduce non-linearity into the model.\n",
    "    - Dropout: Dropout is a regularization technique that randomly drops out a fraction (0.2 in this case) of the neurons during training, which helps prevent overfitting.\n",
    "    - BatchNormalization: Batch normalization normalizes the inputs of each layer to have zero mean and unit variance, which helps stabilize training and improves the learning process.\n",
    "\n",
    "- The output layer is a dense layer with a single neuron, using the sigmoid activation function. Since this is a binary classification problem (spam or not spam), the sigmoid activation function outputs a probability between 0 and 1, indicating the likelihood of an email being spam.\n",
    "\n",
    "- The model is compiled with the Adam optimizer, Binary Crossentropy loss function (suitable for binary classification), and Binary Accuracy metric (used to monitor the accuracy during training).\n",
    "\n",
    "- The model is trained using the fit method with the training data (X_train, y_train). It will undergo 500 epochs (iterations over the entire dataset), and verbose=0 means the training progress won't be printed to the console.\n",
    "\n",
    "- After training, the model is evaluated on both the validation and test datasets. The training accuracy is extracted from the training history, and the evaluation results (loss and accuracy) for the validation and test datasets are obtained.\n",
    "\n",
    "- Then we do the predictions on the test data using the trained model. The predicted probabilities are rounded to obtain binary predictions (0 or 1). The true labels (y_test) are converted to integers (0 or 1). Finally, the classification report is printed for the test data, providing a comprehensive summary of the model's performance in classifying spam emails on the test dataset. The classification report includes metrics like precision, recall, F1-score, and support for both classes (spam and not spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62846507",
   "metadata": {},
   "source": [
    "<a id = \"tf\"></a>\n",
    "### 2.2. TF-IDF Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dfcbc8",
   "metadata": {},
   "source": [
    "Here we are going to train the model created according to the model architecture explained using the dataset that has been TF-IDF vectorised for the email messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45f9597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seeds for reproducibility\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# Create a new sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Declare the hidden layers\n",
    "model.add(layers.Dense(40, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.BatchNormalization()) \n",
    "\n",
    "model.add(layers.Dense(40, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.BatchNormalization()) \n",
    "\n",
    "model.add(layers.Dense(40, activation=\"relu\")) \n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "# Declare the output layer\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),  \n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()]\n",
    ")\n",
    "\n",
    "# Train the network\n",
    "history = model.fit(X_train, y_train, epochs=500, verbose=0)\n",
    "\n",
    "# Evaluate the network\n",
    "train_accuracy = history.history[\"binary_accuracy\"][-1]\n",
    "result1 = model.evaluate(X_validation,y_validation, verbose=0)\n",
    "result2 = model.evaluate(X_test,y_test, verbose=0)\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {result1[1]:.4f}\")\n",
    "print(f\"Test Accuracy: {result2[1]:.4f}\")\n",
    "\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_pred = np.round(y_test_pred).flatten()\n",
    "y_test = np.asarray(y_test, dtype=int)\n",
    "\n",
    "# Print classification report for test data\n",
    "print(\"Classification Report for Test Data:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafdb178",
   "metadata": {},
   "source": [
    "<a id = \"sv\"></a>\n",
    "### 2.3. Sentence2Vec Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d118aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
